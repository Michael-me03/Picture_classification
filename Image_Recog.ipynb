{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMeJzOGG62gaXkzauXzXNRJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Michael-me03/Picture_classification/blob/main/Image_Recog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JjexSbTTNNfj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "a395b268-d7e0-4c5c-c586-f5ec2c85e4d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Reducing dimensions with SVD...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'grid_search' where it is not associated with a value",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-12ad20144ad1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;31m# Test with new images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-12ad20144ad1>\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(images, labels, k, learning_rate, num_iterations)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m# Step 3: Train-Test Split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Step 4:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'grid_search' where it is not associated with a value"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# SVD for Compression\n",
        "def svd_reduction(images, k, target_shape=(64, 64)):\n",
        "    \"\"\"\n",
        "    Applies SVD to reduce the dimensionality of images.\n",
        "\n",
        "    Parameters:\n",
        "        images (list of ndarray): List of images as 2D arrays.\n",
        "        k (int): Number of singular values to keep.\n",
        "        target_shape (tuple): Target shape for resizing images.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: Reduced feature matrix.\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    for image in images:\n",
        "        # Remove the alpha channel if it exists\n",
        "        if image.shape[2] == 4:\n",
        "            image = image[:, :, :3]\n",
        "        # Convert to grayscale if necessary\n",
        "        if image.ndim == 3:\n",
        "            image = rgb2gray(image)\n",
        "        # Resize image to the target shape\n",
        "        image = resize(image, target_shape, anti_aliasing=True)\n",
        "        # Apply SVD\n",
        "        U, S, VT = np.linalg.svd(image, full_matrices=False)\n",
        "        # Keep the first k singular values/vectors\n",
        "        reduced_features = np.dot(U[:, :k], np.diag(S[:k]))\n",
        "        features.append(reduced_features.flatten())  # Flatten the reduced matrix\n",
        "    return np.array(features)\n",
        "\n",
        "# Softmax Function\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "# Classification Pipeline\n",
        "def classify(images, labels, k, learning_rate, num_iterations):\n",
        "    \"\"\"\n",
        "    Pipeline to classify images using SVD and Gradient Descent.\n",
        "\n",
        "    Parameters:\n",
        "        images (list of ndarray): List of input images.\n",
        "        labels (list): Corresponding labels.\n",
        "        k (int): Number of singular values to keep in SVD.\n",
        "        learning_rate (float): Learning rate for Gradient Descent.\n",
        "        num_iterations (int): Number of iterations for Gradient Descent.\n",
        "    \"\"\"\n",
        "    # Step 1: SVD Reduction\n",
        "    print(\"Reducing dimensions with SVD...\")\n",
        "    X = svd_reduction(images, k)\n",
        "    X = X.reshape(X.shape[0], -1)  # Flatten the reduced features\n",
        "\n",
        "    # Step 2: One-Hot Encode Labels\n",
        "    encoder = OneHotEncoder(sparse_output=False)\n",
        "    y = encoder.fit_transform(np.array(labels).reshape(-1, 1))\n",
        "\n",
        "    # Step 3: Train-Test Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=labels)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Step 4:\n",
        "    classifier = SVC()\n",
        "\n",
        "    parameters = [{'gamma': [0.01, 0.001, 0.0001], 'C': [1, 10, 100, 1000]}]\n",
        "\n",
        "    grid_search = GridSearchCV(classifier, parameters)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Test performance\n",
        "    best_estimator = grid_search.best_estimator_\n",
        "    y_prediction = best_estimator.predict(X_test)\n",
        "\n",
        "    score = accuracy_score(y_test, y_prediction)\n",
        "    print('f{}% of samples were correctly classified'.format(str(score * 100)))\n",
        "\n",
        "    # Plot loss history\n",
        "    plt.plot(loss_history)\n",
        "    plt.title(\"Loss Over Iterations\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    # Step 5: Evaluate Model\n",
        "    logits = np.dot(X_test, weights)\n",
        "    probabilities = softmax(logits)\n",
        "    predictions = np.argmax(probabilities, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "    accuracy = np.mean(predictions == y_true)\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "    return weights, encoder\n",
        "\n",
        "# Test Model with New Images\n",
        "def test_model(new_images, weights, k, target_shape=(64, 64), label_mapping=None):\n",
        "    \"\"\"\n",
        "    Tests the trained model on new images.\n",
        "\n",
        "    Parameters:\n",
        "        new_images (list of ndarray): List of new images to test.\n",
        "        weights (ndarray): Trained weights of the model.\n",
        "        k (int): Number of singular values used during training.\n",
        "        target_shape (tuple): Target shape for resizing images.\n",
        "        label_mapping (list): List of original labels corresponding to class indices.\n",
        "\n",
        "    Returns:\n",
        "        list: Predicted labels for the new images.\n",
        "    \"\"\"\n",
        "    # Reduce dimensions of new images using SVD\n",
        "    print(\"Processing new images with SVD...\")\n",
        "    X_new = svd_reduction(new_images, k, target_shape)\n",
        "    X_new = X_new.reshape(X_new.shape[0], -1)  # Flatten the features\n",
        "\n",
        "    # Compute logits and probabilities\n",
        "    logits = np.dot(X_new, weights)\n",
        "    probabilities = softmax(logits)\n",
        "\n",
        "    # Get predicted indices\n",
        "    predictions = np.argmax(probabilities, axis=1)\n",
        "\n",
        "    # Map indices to labels\n",
        "    if label_mapping is not None:\n",
        "        predictions = [label_mapping[idx] for idx in predictions]\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Importing images for training\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "# Load Nazli images (Nazli (1-30))\n",
        "for i in range(1, 80):\n",
        "    image = imread(f'/content/drive/My Drive/Grapevine_Leaves_Image_Dataset/Nazli/Nazli ({i}).png')\n",
        "    images.append(image)\n",
        "    labels.append(\"Nazli\")\n",
        "\n",
        "# Load Dimnit images (Dimnit (1-30))\n",
        "for i in range(1, 80):\n",
        "    image = imread(f'/content/drive/My Drive/Grapevine_Leaves_Image_Dataset/Dimnit/Dimnit ({i}).png')\n",
        "    images.append(image)\n",
        "    labels.append(\"Dimnit\")\n",
        "\n",
        "# Load Buzgulu images (Buzgulu (1-30))\n",
        "for i in range(1, 80):\n",
        "    image = imread(f'/content/drive/My Drive/Grapevine_Leaves_Image_Dataset/Buzgulu/Buzgulu ({i}).png')\n",
        "    images.append(image)\n",
        "    labels.append(\"Buzgulu\")\n",
        "\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Parameters\n",
        "k = 50  # Number of singular values\n",
        "learning_rate = 0.1\n",
        "num_iterations = 25000\n",
        "\n",
        "# Train the model\n",
        "weights, encoder = classify(images, labels, k, learning_rate, num_iterations)\n",
        "\n",
        "# Test with new images\n",
        "new_images = []\n",
        "main_labels = [\"Nazli\"]\n",
        "for i in range(90, 100):\n",
        "    for j in (main_labels):\n",
        "      new_image = imread(f'/content/drive/My Drive/Grapevine_Leaves_Image_Dataset/{j}/{j} ({i}).png')\n",
        "      new_images.append(new_image)\n",
        "\n",
        "if new_images:\n",
        "    label_mapping = encoder.categories_[0]\n",
        "    predicted_labels = test_model(new_images, weights, k, target_shape=(64, 64), label_mapping=label_mapping)\n",
        "\n",
        "    print(\"\\nPredicted labels for new images:\")\n",
        "    for i, label in enumerate(predicted_labels):\n",
        "        print(f\"Image {i + 1}: Predicted Label = {label}\")\n",
        "else:\n",
        "    print(\"No new images found for testing.\")"
      ]
    }
  ]
}