{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JjexSbTTNNfj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "d66bce5f-d836-4630-ded3-e5c8e7d1688f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-12ad20144ad1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# SVD for Compression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# SVD for Compression\n",
        "def svd_reduction(images, k, target_shape=(64, 64)):\n",
        "    \"\"\"\n",
        "    Applies SVD to reduce the dimensionality of images.\n",
        "\n",
        "    Parameters:\n",
        "        images (list of ndarray): List of images as 2D arrays.\n",
        "        k (int): Number of singular values to keep.\n",
        "        target_shape (tuple): Target shape for resizing images.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: Reduced feature matrix.\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    for image in images:\n",
        "        # Remove the alpha channel if it exists\n",
        "        if image.shape[2] == 4:\n",
        "            image = image[:, :, :3]\n",
        "        # Convert to grayscale if necessary\n",
        "        if image.ndim == 3:\n",
        "            image = rgb2gray(image)\n",
        "        # Resize image to the target shape\n",
        "        image = resize(image, target_shape, anti_aliasing=True)\n",
        "        # Apply SVD\n",
        "        U, S, VT = np.linalg.svd(image, full_matrices=False)\n",
        "        # Keep the first k singular values/vectors\n",
        "        reduced_features = np.dot(U[:, :k], np.diag(S[:k]))\n",
        "        features.append(reduced_features.flatten())  # Flatten the reduced matrix\n",
        "    return np.array(features)\n",
        "\n",
        "# Softmax Function\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "# Classification Pipeline\n",
        "def classify(images, labels, k, learning_rate, num_iterations):\n",
        "    \"\"\"\n",
        "    Pipeline to classify images using SVD and Gradient Descent.\n",
        "\n",
        "    Parameters:\n",
        "        images (list of ndarray): List of input images.\n",
        "        labels (list): Corresponding labels.\n",
        "        k (int): Number of singular values to keep in SVD.\n",
        "        learning_rate (float): Learning rate for Gradient Descent.\n",
        "        num_iterations (int): Number of iterations for Gradient Descent.\n",
        "    \"\"\"\n",
        "    # Step 1: SVD Reduction\n",
        "    print(\"Reducing dimensions with SVD...\")\n",
        "    X = svd_reduction(images, k)\n",
        "    X = X.reshape(X.shape[0], -1)  # Flatten the reduced features\n",
        "\n",
        "    # Step 2: One-Hot Encode Labels\n",
        "    encoder = OneHotEncoder(sparse_output=False)\n",
        "    y = encoder.fit_transform(np.array(labels).reshape(-1, 1))\n",
        "\n",
        "    # Step 3: Train-Test Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=labels)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Step 4:\n",
        "    classifier = SVC()\n",
        "\n",
        "    parameters = [{'gamma': [0.01, 0.001, 0.0001], 'C': [1, 10, 100, 1000]}]\n",
        "\n",
        "    grid_search = GridSearchCV(classifier, parameters)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Test performance\n",
        "    best_estimator = grid_search.best_estimator_\n",
        "    y_prediction = best_estimator.predict(X_test)\n",
        "\n",
        "    score = accuracy_score(y_test, y_prediction)\n",
        "    print('f{}% of samples were correctly classified'.format(str(score * 100)))\n",
        "\n",
        "    # Plot loss history\n",
        "    plt.plot(loss_history)\n",
        "    plt.title(\"Loss Over Iterations\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    # Step 5: Evaluate Model\n",
        "    logits = np.dot(X_test, weights)\n",
        "    probabilities = softmax(logits)\n",
        "    predictions = np.argmax(probabilities, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "    accuracy = np.mean(predictions == y_true)\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "    return weights, encoder\n",
        "\n",
        "# Test Model with New Images\n",
        "def test_model(new_images, weights, k, target_shape=(64, 64), label_mapping=None):\n",
        "    \"\"\"\n",
        "    Tests the trained model on new images.\n",
        "\n",
        "    Parameters:\n",
        "        new_images (list of ndarray): List of new images to test.\n",
        "        weights (ndarray): Trained weights of the model.\n",
        "        k (int): Number of singular values used during training.\n",
        "        target_shape (tuple): Target shape for resizing images.\n",
        "        label_mapping (list): List of original labels corresponding to class indices.\n",
        "\n",
        "    Returns:\n",
        "        list: Predicted labels for the new images.\n",
        "    \"\"\"\n",
        "    # Reduce dimensions of new images using SVD\n",
        "    print(\"Processing new images with SVD...\")\n",
        "    X_new = svd_reduction(new_images, k, target_shape)\n",
        "    X_new = X_new.reshape(X_new.shape[0], -1)  # Flatten the features\n",
        "\n",
        "    # Compute logits and probabilities\n",
        "    logits = np.dot(X_new, weights)\n",
        "    probabilities = softmax(logits)\n",
        "\n",
        "    # Get predicted indices\n",
        "    predictions = np.argmax(probabilities, axis=1)\n",
        "\n",
        "    # Map indices to labels\n",
        "    if label_mapping is not None:\n",
        "        predictions = [label_mapping[idx] for idx in predictions]\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Importing images for training\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "# Load Nazli images (Nazli (1-30))\n",
        "for i in range(1, 80):\n",
        "    image = imread(f'/content/drive/My Drive/Grapevine_Leaves_Image_Dataset/Nazli/Nazli ({i}).png')\n",
        "    images.append(image)\n",
        "    labels.append(\"Nazli\")\n",
        "\n",
        "# Load Dimnit images (Dimnit (1-30))\n",
        "for i in range(1, 80):\n",
        "    image = imread(f'/content/drive/My Drive/Grapevine_Leaves_Image_Dataset/Dimnit/Dimnit ({i}).png')\n",
        "    images.append(image)\n",
        "    labels.append(\"Dimnit\")\n",
        "\n",
        "# Load Buzgulu images (Buzgulu (1-30))\n",
        "for i in range(1, 80):\n",
        "    image = imread(f'/content/drive/My Drive/Grapevine_Leaves_Image_Dataset/Buzgulu/Buzgulu ({i}).png')\n",
        "    images.append(image)\n",
        "    labels.append(\"Buzgulu\")\n",
        "\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Parameters\n",
        "k = 50  # Number of singular values\n",
        "learning_rate = 0.1\n",
        "num_iterations = 25000\n",
        "\n",
        "# Train the model\n",
        "weights, encoder = classify(images, labels, k, learning_rate, num_iterations)\n",
        "\n",
        "# Test with new images\n",
        "new_images = []\n",
        "main_labels = [\"Nazli\"]\n",
        "for i in range(90, 100):\n",
        "    for j in (main_labels):\n",
        "      new_image = imread(f'/content/drive/My Drive/Grapevine_Leaves_Image_Dataset/{j}/{j} ({i}).png')\n",
        "      new_images.append(new_image)\n",
        "\n",
        "if new_images:\n",
        "    label_mapping = encoder.categories_[0]\n",
        "    predicted_labels = test_model(new_images, weights, k, target_shape=(64, 64), label_mapping=label_mapping)\n",
        "\n",
        "    print(\"\\nPredicted labels for new images:\")\n",
        "    for i, label in enumerate(predicted_labels):\n",
        "        print(f\"Image {i + 1}: Predicted Label = {label}\")\n",
        "else:\n",
        "    print(\"No new images found for testing.\")"
      ]
    }
  ]
}